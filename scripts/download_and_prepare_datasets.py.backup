#!/usr/bin/env python3
"""
Download and prepare benchmark datasets for DeepResearch evaluation.

Datasets:
- GAIA: Question answering with file attachments (requires HuggingFace approval)
- HLE: Humanity's Last Exam
- BrowseComp: Web browsing comprehension

Usage:
    python download_and_prepare_datasets.py --sample_size 70 --output_dir inference/eval_data
"""

import argparse
import json
import os
import random
from pathlib import Path
from typing import List, Dict
import shutil

def setup_directories(output_dir: str):
    """Create necessary directories for datasets."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    file_corpus = output_path / "file_corpus"
    file_corpus.mkdir(exist_ok=True)

    print(f"✓ Created output directory: {output_path}")
    print(f"✓ Created file corpus directory: {file_corpus}")
    return output_path, file_corpus


def download_gaia(output_dir: Path, file_corpus: Path, sample_size: int):
    """Download and prepare GAIA dataset."""
    print("\n" + "="*60)
    print("Downloading GAIA Dataset")
    print("="*60)

    try:
        from datasets import load_dataset
        from huggingface_hub import snapshot_download

        print("Attempting to download GAIA dataset...")
        print("Note: This dataset requires approval from HuggingFace.")
        print("      If download fails, please request access at:")
        print("      https://huggingface.co/datasets/gaia-benchmark/GAIA")
        print()

        # Try to download the dataset
        try:
            # Download validation split for testing
            dataset = load_dataset("gaia-benchmark/GAIA", "2023_all", split="validation")
            print(f"✓ Successfully loaded GAIA dataset: {len(dataset)} examples")

            # Sample questions
            if len(dataset) > sample_size:
                indices = random.sample(range(len(dataset)), sample_size)
                dataset = dataset.select(indices)
                print(f"✓ Sampled {sample_size} questions")

            # Convert to JSONL format
            output_file = output_dir / "gaia_test.jsonl"
            with open(output_file, 'w', encoding='utf-8') as f:
                for item in dataset:
                    # Handle file attachments
                    question = item['Question']
                    if 'file_name' in item and item['file_name']:
                        # Prepend filename to question
                        question = f"{item['file_name']} {question}"
                        print(f"  - Question with file: {item['file_name']}")

                    entry = {
                        "question": question,
                        "answer": item['Final answer']
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')

            print(f"✓ Saved GAIA dataset to: {output_file}")
            print(f"  Total questions: {len(dataset)}")

            # Note about file downloads
            print("\n⚠️  File Attachments:")
            print("    GAIA includes file attachments (PDFs, images, etc.)")
            print("    You need to manually download them from:")
            print("    https://huggingface.co/datasets/gaia-benchmark/GAIA/tree/main/2023/validation")
            print(f"    Place them in: {file_corpus}/")

            return True

        except Exception as e:
            print(f"✗ Failed to download GAIA: {e}")
            print("\n  Possible reasons:")
            print("  1. You need to request access at https://huggingface.co/datasets/gaia-benchmark/GAIA")
            print("  2. You need to login: huggingface-cli login")
            print("  3. Dataset structure may have changed")
            print("\n  Skipping GAIA for now...")
            return False

    except ImportError:
        print("✗ Error: 'datasets' package not installed")
        print("  Run: pip install datasets")
        return False


def download_hle(output_dir: Path, sample_size: int):
    """Download and prepare HLE dataset."""
    print("\n" + "="*60)
    print("Downloading HLE Dataset")
    print("="*60)

    try:
        from datasets import load_dataset

        print("Downloading HLE (Humanity's Last Exam) dataset...")

        # Download test split
        dataset = load_dataset("cais/hle", split="test")
        print(f"✓ Successfully loaded HLE dataset: {len(dataset)} examples")

        # Sample questions
        if len(dataset) > sample_size:
            indices = random.sample(range(len(dataset)), sample_size)
            dataset = dataset.select(indices)
            print(f"✓ Sampled {sample_size} questions")

        # Convert to JSONL format
        output_file = output_dir / "hle_test.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in dataset:
                entry = {
                    "question": item['question'],
                    "answer": item.get('answer', '')  # HLE may have empty answers
                }
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')

        print(f"✓ Saved HLE dataset to: {output_file}")
        print(f"  Total questions: {len(dataset)}")

        return True

    except Exception as e:
        print(f"✗ Failed to download HLE: {e}")
        print("  Skipping HLE for now...")
        return False


def download_browsecomp(output_dir: Path, sample_size: int):
    """Download and prepare BrowseComp dataset."""
    print("\n" + "="*60)
    print("Downloading BrowseComp Dataset")
    print("="*60)

    try:
        from datasets import load_dataset

        print("Downloading BrowseComp-EN dataset...")

        # Try to load BrowseComp
        # Note: The exact dataset path may vary
        try:
            dataset = load_dataset("Alibaba-NLP/BrowseComp", "en", split="test")
        except:
            print("  Trying alternative dataset path...")
            # If not available on HuggingFace, create a synthetic sample
            print("  ⚠️  BrowseComp not found on HuggingFace")
            print("  Creating sample questions based on web search tasks...")

            # Create synthetic BrowseComp-style questions
            synthetic_questions = [
                {
                    "question": "What is the current population of Tokyo according to official statistics?",
                    "answer": ""
                },
                {
                    "question": "Who won the Nobel Prize in Physics in 2023 and what was their contribution?",
                    "answer": ""
                },
                {
                    "question": "What are the main ingredients in a traditional Italian carbonara recipe?",
                    "answer": ""
                },
                {
                    "question": "When was the Eiffel Tower completed and how tall is it?",
                    "answer": ""
                },
                {
                    "question": "What is the latest version of Python as of 2024 and when was it released?",
                    "answer": ""
                },
            ]

            # Expand to sample_size
            dataset = synthetic_questions * (sample_size // len(synthetic_questions) + 1)
            dataset = dataset[:sample_size]

            output_file = output_dir / "browsecomp_test.jsonl"
            with open(output_file, 'w', encoding='utf-8') as f:
                for item in dataset:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')

            print(f"✓ Created synthetic BrowseComp dataset: {output_file}")
            print(f"  Total questions: {len(dataset)}")
            print("  Note: These are placeholder questions for testing")
            return True

        print(f"✓ Successfully loaded BrowseComp dataset: {len(dataset)} examples")

        # Sample questions
        if len(dataset) > sample_size:
            indices = random.sample(range(len(dataset)), sample_size)
            dataset = dataset.select(indices)
            print(f"✓ Sampled {sample_size} questions")

        # Convert to JSONL format
        output_file = output_dir / "browsecomp_test.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in dataset:
                entry = {
                    "question": item['question'],
                    "answer": item.get('answer', '')
                }
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')

        print(f"✓ Saved BrowseComp dataset to: {output_file}")
        print(f"  Total questions: {len(dataset)}")

        return True

    except Exception as e:
        print(f"✗ Failed to download BrowseComp: {e}")
        print("  Skipping BrowseComp for now...")
        return False


def create_small_test_set(output_dir: Path):
    """Create a small 5-question test set for initial testing."""
    print("\n" + "="*60)
    print("Creating Small Test Set (5 questions)")
    print("="*60)

    test_questions = [
        {
            "question": "What is the capital of France?",
            "answer": "Paris"
        },
        {
            "question": "Who wrote 'Romeo and Juliet'?",
            "answer": "William Shakespeare"
        },
        {
            "question": "What is 15 multiplied by 8?",
            "answer": "120"
        },
        {
            "question": "In what year did World War II end?",
            "answer": "1945"
        },
        {
            "question": "What is the chemical symbol for gold?",
            "answer": "Au"
        }
    ]

    output_file = output_dir / "test_small.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in test_questions:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"✓ Created small test set: {output_file}")
    print("  Use this for initial setup verification")


def main():
    parser = argparse.ArgumentParser(description="Download and prepare benchmark datasets")
    parser.add_argument("--sample_size", type=int, default=70,
                       help="Number of questions to sample from each dataset (default: 70)")
    parser.add_argument("--output_dir", type=str, default="inference/eval_data",
                       help="Output directory for datasets (default: inference/eval_data)")
    parser.add_argument("--datasets", nargs="+", default=["gaia", "hle", "browsecomp"],
                       choices=["gaia", "hle", "browsecomp", "all"],
                       help="Which datasets to download (default: all)")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed for sampling (default: 42)")

    args = parser.parse_args()

    # Set random seed
    random.seed(args.seed)

    print("="*60)
    print("DeepResearch Dataset Download and Preparation")
    print("="*60)
    print(f"Sample size per dataset: {args.sample_size}")
    print(f"Output directory: {args.output_dir}")
    print(f"Random seed: {args.seed}")

    # Setup directories
    output_dir, file_corpus = setup_directories(args.output_dir)

    # Create small test set
    create_small_test_set(output_dir)

    # Download datasets
    datasets_to_download = args.datasets
    if "all" in datasets_to_download:
        datasets_to_download = ["gaia", "hle", "browsecomp"]

    results = {}

    if "gaia" in datasets_to_download:
        results["gaia"] = download_gaia(output_dir, file_corpus, args.sample_size)

    if "hle" in datasets_to_download:
        results["hle"] = download_hle(output_dir, args.sample_size)

    if "browsecomp" in datasets_to_download:
        results["browsecomp"] = download_browsecomp(output_dir, args.sample_size)

    # Summary
    print("\n" + "="*60)
    print("Summary")
    print("="*60)
    print("Downloaded datasets:")
    for dataset, success in results.items():
        status = "✓" if success else "✗"
        print(f"  {status} {dataset.upper()}")

    print("\nNext steps:")
    print("1. Review the API_KEYS_SETUP.md file for required API keys")
    print("2. Fill in your API keys in the .env file")
    print("3. If using GAIA, download file attachments manually")
    print("4. Run initial test with: bash inference/run_react_infer_openrouter.sh")
    print("   (Update DATASET in .env to: inference/eval_data/test_small.jsonl)")


if __name__ == "__main__":
    main()
